{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-S tree logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# used in 1.3 and 4.1\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# used in 4.2\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# used in 5\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../data/splited/train.csv')\n",
    "test = pd.read_csv('../../data/splited/test.csv')\n",
    "train_label = train.iloc[:, 1]\n",
    "train_features = train.iloc[:, 2 :]\n",
    "test_label = test.iloc[:, 1]\n",
    "test_features = test.iloc[:, 2 :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with `cat` as postfix in names are categorical features. Features with `bin` as postfix in names are binary features. Features without postfix in names are continuous features.\n",
    "\n",
    "Features are divided into 4 groups: `ind`, `reg`, `car`, `calc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
       "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
       "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
       "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
       "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
       "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
       "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
       "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
       "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
       "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
       "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
       "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
       "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of `-1` mean missing values. Codes below deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"-1\" with \"np.nan\"\n",
    "train_features = train_features.replace(-1, np.nan)\n",
    "test_features = test_features.replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing ratio in train_features:\n",
      "\n",
      "ps_car_03_cat    0.690852\n",
      "ps_car_05_cat    0.447769\n",
      "ps_reg_03        0.181335\n",
      "ps_car_14        0.071324\n",
      "ps_car_07_cat    0.019066\n",
      "ps_ind_05_cat    0.009773\n",
      "ps_car_09_cat    0.000931\n",
      "ps_ind_02_cat    0.000343\n",
      "ps_car_01_cat    0.000178\n",
      "ps_ind_04_cat    0.000137\n",
      "ps_car_11        0.000007\n",
      "ps_car_02_cat    0.000005\n",
      "ps_car_12        0.000002\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ratio of missing values in train_features\n",
    "train_features_missing_ratio = train_features.isnull().mean()\n",
    "train_features_missing_ratio = train_features_missing_ratio[train_features_missing_ratio > 0] # only consider features with non-zero missing ratio\n",
    "train_features_top_missing_ratio = train_features_missing_ratio.sort_values(ascending=False)\n",
    "print(\"missing ratio in train_features:\\n\")\n",
    "print(train_features_top_missing_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing ratio in train_features:\n",
      "\n",
      "ps_car_03_cat    0.691007\n",
      "ps_car_05_cat    0.447957\n",
      "ps_reg_03        0.180434\n",
      "ps_car_14        0.072260\n",
      "ps_car_07_cat    0.019853\n",
      "ps_ind_05_cat    0.009728\n",
      "ps_car_09_cat    0.001014\n",
      "ps_ind_02_cat    0.000409\n",
      "ps_car_01_cat    0.000185\n",
      "ps_ind_04_cat    0.000146\n",
      "ps_car_02_cat    0.000017\n",
      "ps_car_11        0.000011\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ratio of missing values in test_features\n",
    "test_features_missing_ratio = test_features.isnull().mean()\n",
    "test_features_missing_ratio = test_features_missing_ratio[test_features_missing_ratio > 0] # only consider features with non-zero missing ratio\n",
    "test_features_top_missing_ratio = test_features_missing_ratio.sort_values(ascending=False)\n",
    "print(\"missing ratio in train_features:\\n\")\n",
    "print(test_features_top_missing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features `ps_car_03_cat`, `ps_car_05_cat` and `ps_reg_03` have too high missing ratio in both training and testing set, directedly delete these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ps_car_03_cat', 'ps_car_05_cat', 'ps_reg_03']\n",
    "train_features = train_features.drop(columns=columns_to_drop, errors='ignore') # ignore error if columns to delete don't exist\n",
    "test_features = test_features.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other features have missing ratio lower than 0.08. Fill continuous features with medieans of these features. Fill categorical and binary features with modes of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of features with low missing radio\n",
    "columns_to_fill_with_mode = ['ps_car_07_cat', 'ps_ind_05_cat', 'ps_car_09_cat',\n",
    "                             'ps_ind_02_cat', 'ps_car_01_cat', 'ps_ind_04_cat',\n",
    "                             'ps_car_02_cat']\n",
    "columns_to_fill_with_median = ['ps_car_14', 'ps_car_11', 'ps_car_12']\n",
    "\n",
    "# training set\n",
    "train_features[columns_to_fill_with_mode] = train_features[columns_to_fill_with_mode].fillna(\n",
    "    train_features[columns_to_fill_with_mode].mode().iloc[0]\n",
    ")\n",
    "train_features[columns_to_fill_with_median] = train_features[columns_to_fill_with_median].fillna(\n",
    "    train_features[columns_to_fill_with_median].median()\n",
    ")\n",
    "\n",
    "\n",
    "# testing set\n",
    "test_features[columns_to_fill_with_mode] = test_features[columns_to_fill_with_mode].fillna(\n",
    "    test_features[columns_to_fill_with_mode].mode().iloc[0]\n",
    ")\n",
    "test_features[columns_to_fill_with_median] = test_features[columns_to_fill_with_median].fillna(\n",
    "    test_features[columns_to_fill_with_median].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 standardization and encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of continuous features\n",
    "continuous_features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15',\n",
    "                       'ps_reg_01', 'ps_reg_02', 'ps_car_11', 'ps_car_12',\n",
    "                       'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
    "                       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05',\n",
    "                       'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09',\n",
    "                       'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_continuous = scaler.fit_transform(train_features[continuous_features])\n",
    "test_continuous = scaler.transform(test_features[continuous_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical features using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of categorical features\n",
    "categorical_features = ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat','ps_car_01_cat',\n",
    "                        'ps_car_02_cat', 'ps_car_04_cat', 'ps_car_06_cat', 'ps_car_07_cat',\n",
    "                        'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "train_categorical = encoder.fit_transform(train_features[categorical_features])\n",
    "test_categorical = encoder.transform(test_features[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary features remain fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of binary features\n",
    "binary_features = ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin',\n",
    "                   'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin',\n",
    "                   'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_calc_15_bin',\n",
    "                   'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary = train_features[binary_features].values\n",
    "test_binary = test_features[binary_features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate three kinds of features into final traning and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_processed = np.hstack((train_continuous, train_categorical, train_binary))\n",
    "test_features_processed = np.hstack((test_continuous, test_categorical, test_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables `train_features_processed` and `test_features_processed` are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (416648, 213))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_features_processed), train_features_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (178564, 213))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_features_processed), test_features_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDA\n",
    "\n",
    "Numpy arrays `train_features_processed` and `test_features_processed`, obtained in section 1.3 in this notebook, can be used here.\n",
    "\n",
    "According setion 3 of the \"property refinance prediction\" paper, EDA should focus on the calculation of the complexity (overlapping area) of unsplitted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-S static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 K-S static calculation\n",
    "\n",
    "Calculating K-S static has no requirement of standardization and encoding. So dataframes `train_features`, obtained in section 1.2 in this notebook, can be used here.\n",
    "\n",
    "Calculating K-S static requires label information. So dataframes `train_label`, obtained in section 1.1 in this notebook, can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature_name  ks_statistic  max_diff_feature_value\n",
      "31       ps_car_13      0.114100                0.832079\n",
      "5    ps_ind_06_bin      0.088906                0.000000\n",
      "19       ps_reg_02      0.087595                0.400000\n",
      "6    ps_ind_07_bin      0.079796                0.000000\n",
      "30       ps_car_12      0.077120                0.374166\n",
      "22   ps_car_04_cat      0.073953                0.000000\n",
      "33       ps_car_15      0.071902                3.316625\n",
      "15   ps_ind_16_bin      0.070316                0.000000\n",
      "20   ps_car_01_cat      0.067117                7.000000\n",
      "16   ps_ind_17_bin      0.065833                1.000000\n",
      "21   ps_car_02_cat      0.063247                0.000000\n",
      "18       ps_reg_01      0.060861                0.600000\n",
      "14       ps_ind_15      0.058136                7.000000\n",
      "0        ps_ind_01      0.052497                2.000000\n",
      "4    ps_ind_05_cat      0.050838                0.000000\n",
      "2        ps_ind_03      0.045410                5.000000\n",
      "25   ps_car_08_cat      0.039485                0.000000\n",
      "28   ps_car_11_cat      0.038128              104.000000\n",
      "32       ps_car_14      0.036837                0.398497\n",
      "23   ps_car_06_cat      0.030343               11.000000\n",
      "26   ps_car_09_cat      0.029334                1.000000\n",
      "3    ps_ind_04_cat      0.027944                0.000000\n",
      "7    ps_ind_08_bin      0.026425                0.000000\n",
      "24   ps_car_07_cat      0.021913                0.000000\n",
      "8    ps_ind_09_bin      0.017315                0.000000\n",
      "1    ps_ind_02_cat      0.015518                1.000000\n",
      "29       ps_car_11      0.013529                2.000000\n",
      "40      ps_calc_07      0.010699                4.000000\n",
      "17   ps_ind_18_bin      0.008415                1.000000\n",
      "35      ps_calc_02      0.008141                0.300000\n",
      "41      ps_calc_08      0.007742                9.000000\n",
      "36      ps_calc_03      0.007378                0.100000\n",
      "13       ps_ind_14      0.007354                0.000000\n",
      "42      ps_calc_09      0.006957                1.000000\n",
      "47      ps_calc_14      0.006454                5.000000\n",
      "45      ps_calc_12      0.006448                2.000000\n",
      "43      ps_calc_10      0.006389                7.000000\n",
      "34      ps_calc_01      0.006310                0.200000\n",
      "38      ps_calc_05      0.006194                2.000000\n",
      "46      ps_calc_13      0.006064                4.000000\n",
      "37      ps_calc_04      0.005938                2.000000\n",
      "44      ps_calc_11      0.005360                6.000000\n",
      "39      ps_calc_06      0.004627                8.000000\n",
      "11   ps_ind_12_bin      0.004174                0.000000\n",
      "52  ps_calc_19_bin      0.003284                0.000000\n",
      "49  ps_calc_16_bin      0.002855                0.000000\n",
      "51  ps_calc_18_bin      0.002267                0.000000\n",
      "53  ps_calc_20_bin      0.001663                0.000000\n",
      "48  ps_calc_15_bin      0.000989                0.000000\n",
      "10   ps_ind_11_bin      0.000711                0.000000\n",
      "27   ps_car_10_cat      0.000525                0.000000\n",
      "9    ps_ind_10_bin      0.000281                1.000000\n",
      "12   ps_ind_13_bin      0.000272                1.000000\n",
      "50  ps_calc_17_bin      0.000126                0.000000\n"
     ]
    }
   ],
   "source": [
    "ks_results = []\n",
    "\n",
    "for feature_name in train_features.columns:\n",
    "    # construct dataframe\n",
    "    temp_data = {\n",
    "        'feature': list(train_features[feature_name].values),\n",
    "        'target': list(train_label.values)\n",
    "    }\n",
    "    temp_df = pd.DataFrame(temp_data)\n",
    "    temp_df = temp_df.sort_values(by='feature')\n",
    "    # calculate ks_statistic of the given feature\n",
    "    if 'cat' not in str(feature_name) and 'bin' not in str(feature_name):\n",
    "        temp_df['cum_positive'] = (temp_df['target'] == 1).cumsum() / (temp_df['target'] == 1).sum() # cdf of label 1\n",
    "        temp_df['cum_negative'] = (temp_df['target'] == 0).cumsum() / (temp_df['target'] == 0).sum() # cdf of label 0\n",
    "        temp_df['diff'] = np.abs(temp_df['cum_positive'] - temp_df['cum_negative'])\n",
    "        ks_statistic = temp_df['diff'].max()\n",
    "        max_diff_feature_value = temp_df.loc[temp_df['diff'].idxmax(), 'feature'] # value of the continuous feature at which maximum cdf difference is achieved\n",
    "    else:\n",
    "        category_binary_stats = temp_df.groupby('feature')['target'].value_counts(normalize=False).unstack(fill_value=0)\n",
    "        label_counts = temp_df['target'].value_counts()\n",
    "        category_binary_stats[0] = category_binary_stats[0] / label_counts[0]\n",
    "        category_binary_stats[1] = category_binary_stats[1] / label_counts[1]\n",
    "        category_binary_stats['abs_diff'] = (category_binary_stats[0] - category_binary_stats[1]).abs()\n",
    "        ks_statistic = category_binary_stats['abs_diff'].max()\n",
    "        max_diff_feature_value = category_binary_stats['abs_diff'].idxmax() # value of the categorical/binary feature at which maximum proportion difference is achieved\n",
    "\n",
    "    ks_results.append({'feature_name': feature_name, 'ks_statistic': ks_statistic, 'max_diff_feature_value': max_diff_feature_value})\n",
    "\n",
    "ks_df = pd.DataFrame(ks_results)\n",
    "ks_df_sorted = ks_df.sort_values(by='ks_statistic', ascending=False)\n",
    "print(ks_df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 feature selection and data segementation via K-S statistc\n",
    "\n",
    "Calculated K-S statistic is stored in a dataframe named `ks_df_sorted`. Select features with K-S statistic higher than 0.08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>ks_statistic</th>\n",
       "      <th>max_diff_feature_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ps_car_13</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.832079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ps_ind_06_bin</td>\n",
       "      <td>0.088906</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ps_reg_02</td>\n",
       "      <td>0.087595</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_name  ks_statistic  max_diff_feature_value\n",
       "31      ps_car_13      0.114100                0.832079\n",
       "5   ps_ind_06_bin      0.088906                0.000000\n",
       "19      ps_reg_02      0.087595                0.400000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = ks_df_sorted[ks_df_sorted['ks_statistic'] > 0.08]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three features are selected: `ps_car_13`, `ps_ind_06_bin`, `ps_reg_02`. The whole training data is first segemented into `ps_car_13 <= 0.832079` and `ps_car_13 > 0.832079`, resulting in 2 segemented training data. The 2 segemented training data are then segemented into `ps_ind_06_bin == 1` and `ps_ind_06_bin == 0`, resulting in 4 segemented training data. The 4 segemented training data are again segemented into `ps_reg_02 <= 0.4` and `ps_reg_02 > 0.4`, resulting in 8 segemented training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_1 = train_features[(train_features['ps_car_13'] <= 0.832079) & (train_features['ps_ind_06_bin'] == 1) & (train_features['ps_reg_02'] <= 0.4)]\n",
    "train_label_1 = train_label[train_features_1.index]\n",
    "\n",
    "train_features_2 = train_features[(train_features['ps_car_13'] <= 0.832079) & (train_features['ps_ind_06_bin'] == 1) & (train_features['ps_reg_02'] > 0.4)]\n",
    "train_label_2 = train_label[train_features_2.index]\n",
    "\n",
    "train_features_3 = train_features[(train_features['ps_car_13'] <= 0.832079) & (train_features['ps_ind_06_bin'] == 0) & (train_features['ps_reg_02'] <= 0.4)]\n",
    "train_label_3 = train_label[train_features_3.index]\n",
    "\n",
    "train_features_4 = train_features[(train_features['ps_car_13'] <= 0.832079) & (train_features['ps_ind_06_bin'] == 0) & (train_features['ps_reg_02'] > 0.4)]\n",
    "train_label_4 = train_label[train_features_4.index]\n",
    "\n",
    "train_features_5 = train_features[(train_features['ps_car_13'] > 0.832079) & (train_features['ps_ind_06_bin'] == 1) & (train_features['ps_reg_02'] <= 0.4)]\n",
    "train_label_5 = train_label[train_features_5.index]\n",
    "\n",
    "train_features_6 = train_features[(train_features['ps_car_13'] > 0.832079) & (train_features['ps_ind_06_bin'] == 1) & (train_features['ps_reg_02'] > 0.4)]\n",
    "train_label_6 = train_label[train_features_6.index]\n",
    "\n",
    "train_features_7 = train_features[(train_features['ps_car_13'] > 0.832079) & (train_features['ps_ind_06_bin'] == 0) & (train_features['ps_reg_02'] <= 0.4)]\n",
    "train_label_7 = train_label[train_features_7.index]\n",
    "\n",
    "train_features_8 = train_features[(train_features['ps_car_13'] > 0.832079) & (train_features['ps_ind_06_bin'] == 0) & (train_features['ps_reg_02'] > 0.4)]\n",
    "train_label_8 = train_label[train_features_8.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. data segements processing, resampling and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 data segments processing\n",
    "\n",
    "This is similar to section 1.3 in this notebook. In 8 data segements `train_features_1` ~ `train_features_8`, continuous features shoud be standardized and categorial features should be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_segements = [train_features_1, train_features_2, train_features_3, train_features_4,\n",
    "                            train_features_5, train_features_6, train_features_7, train_features_8]\n",
    "train_features_segements_processed = []\n",
    "\n",
    "# names of continuous features\n",
    "continuous_features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15',\n",
    "                       'ps_reg_01', 'ps_reg_02', 'ps_car_11', 'ps_car_12',\n",
    "                       'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
    "                       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05',\n",
    "                       'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09',\n",
    "                       'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14']\n",
    "# names of categorical features\n",
    "categorical_features = ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat','ps_car_01_cat',\n",
    "                        'ps_car_02_cat', 'ps_car_04_cat', 'ps_car_06_cat', 'ps_car_07_cat',\n",
    "                        'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat']\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(train_features[categorical_features]) # fit one-hot encoder on the entire training set to avoid different number of features in data_segements_processed\n",
    "# names of binary features\n",
    "binary_features = ['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin',\n",
    "                   'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin',\n",
    "                   'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_calc_15_bin',\n",
    "                   'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin']\n",
    "\n",
    "for train_features_seg in train_features_segements:\n",
    "    # standardize continuous features\n",
    "    scaler = StandardScaler()\n",
    "    train_features_seg_continuous = scaler.fit_transform(train_features_seg[continuous_features])\n",
    "    \n",
    "    # encode categorical features\n",
    "    train_features_seg_categorical = encoder.transform(train_features_seg[categorical_features])\n",
    "\n",
    "    # binary features remain fixed\n",
    "    train_features_seg_binary = train_features_seg[binary_features].values\n",
    "\n",
    "    # integrate processed data segement into a numpy array\n",
    "    train_features_seg_processed = np.hstack((train_features_seg_continuous, train_features_seg_categorical, train_features_seg_binary))\n",
    "    train_features_segements_processed.append(train_features_seg_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have processed training features segements: `train_features_segements_processed[0]` ~ `train_features_segements_processed[7]`, which are all numpy arrays. We also have corresponding training lables segements: `train_label_1` ~ `train_label_8`, which are all dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 resampling and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling means over-sampling minority labels and down-sampling marjority labels.\n",
    "\n",
    "The following codes use `SMOTE`, which is not mentioned in the paper. The goal to use it here is to get model performance on testing set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_segements = [train_label_1, train_label_2, train_label_3, train_label_4,\n",
    "                         train_label_5, train_label_6, train_label_7, train_label_8]\n",
    "\n",
    "# train a logistic regression model on every data segement\n",
    "pipelines = []\n",
    "for i in range(len(train_label_segements)):\n",
    "    pipeline = Pipeline([\n",
    "        #('smote', SMOTE(random_state=42)),\n",
    "        #('model', LogisticRegression(penalty='none', max_iter=50, class_weight='balanced'))\n",
    "        ('model', RandomForestClassifier(max_depth=2, random_state=0))\n",
    "    ])\n",
    "    pipeline.fit(train_features_segements_processed[i], train_label_segements[i])\n",
    "    pipelines.append(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. model performance on testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 1.3 in this notebook, the test features have been standardized and one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (178564, 213))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_features_processed), test_features_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices of different segements can be found via the original `test_features` in section 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_1 = test_features[(test_features['ps_car_13'] <= 0.832079) & (test_features['ps_ind_06_bin'] == 1) & (test_features['ps_reg_02'] <= 0.4)].index\n",
    "indices_2 = test_features[(test_features['ps_car_13'] <= 0.832079) & (test_features['ps_ind_06_bin'] == 1) & (test_features['ps_reg_02'] > 0.4)].index\n",
    "indices_3 = test_features[(test_features['ps_car_13'] <= 0.832079) & (test_features['ps_ind_06_bin'] == 0) & (test_features['ps_reg_02'] <= 0.4)].index\n",
    "indices_4 = test_features[(test_features['ps_car_13'] <= 0.832079) & (test_features['ps_ind_06_bin'] == 0) & (test_features['ps_reg_02'] > 0.4)].index\n",
    "indices_5 = test_features[(test_features['ps_car_13'] > 0.832079) & (test_features['ps_ind_06_bin'] == 1) & (test_features['ps_reg_02'] <= 0.4)].index\n",
    "indices_6 = test_features[(test_features['ps_car_13'] > 0.832079) & (test_features['ps_ind_06_bin'] == 1) & (test_features['ps_reg_02'] > 0.4)].index\n",
    "indices_7 = test_features[(test_features['ps_car_13'] > 0.832079) & (test_features['ps_ind_06_bin'] == 0) & (test_features['ps_reg_02'] <= 0.4)].index\n",
    "indices_8 = test_features[(test_features['ps_car_13'] > 0.832079) & (test_features['ps_ind_06_bin'] == 0) & (test_features['ps_reg_02'] > 0.4)].index\n",
    "indices_set = [indices_1, indices_2, indices_3, indices_4,\n",
    "               indices_5, indices_6, indices_7, indices_8]\n",
    "\n",
    "test_label_prediction = []\n",
    "test_label_reordered = []\n",
    "for i in range(8):\n",
    "    test_label_prediction.append(pipelines[i].predict_proba(test_features_processed[indices_set[i]])[:, 1]) # logistic regression & random forest\n",
    "    test_label_reordered.append(test_label[indices_set[i]])\n",
    "test_label_prediction = np.concatenate(test_label_prediction, axis=0)\n",
    "test_label_reordered = np.concatenate(test_label_reordered, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6004632633856525\n"
     ]
    }
   ],
   "source": [
    "auc_score = roc_auc_score(test_label_reordered, test_label_prediction)\n",
    "print(f\"AUC: {auc_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
