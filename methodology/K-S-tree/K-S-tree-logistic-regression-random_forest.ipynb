{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-S tree logistic regression & random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# performance metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/original/train.csv')\n",
    "train_features, test_features, train_label, test_label = train_test_split(data.iloc[:, 2 :], data.iloc[:, 1], test_size = 0.2, stratify = data.iloc[:, 1], random_state = 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with `cat` as postfix in names are categorical features. Features with `bin` as postfix in names are binary features. Features without postfix in names are continuous features.\n",
    "\n",
    "Features are divided into 4 groups: `ind`, `reg`, `car`, `calc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
       "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
       "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
       "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
       "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
       "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
       "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
       "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
       "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
       "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
       "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
       "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
       "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of `-1` mean missing values. Codes below deal with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"-1\" with \"np.nan\"\n",
    "train_features = train_features.replace(-1, np.nan)\n",
    "test_features = test_features.replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing ratio in train_features:\n",
      "\n",
      "ps_car_03_cat    0.690587\n",
      "ps_car_05_cat    0.447329\n",
      "ps_reg_03        0.180820\n",
      "ps_car_14        0.071420\n",
      "ps_car_07_cat    0.019373\n",
      "ps_ind_05_cat    0.009868\n",
      "ps_car_09_cat    0.000962\n",
      "ps_ind_02_cat    0.000380\n",
      "ps_car_01_cat    0.000181\n",
      "ps_ind_04_cat    0.000141\n",
      "ps_car_11        0.000006\n",
      "ps_car_02_cat    0.000004\n",
      "ps_car_12        0.000002\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ratio of missing values in train_features\n",
    "train_features_missing_ratio = train_features.isnull().mean()\n",
    "train_features_missing_ratio = train_features_missing_ratio[train_features_missing_ratio > 0] # only consider features with non-zero missing ratio\n",
    "train_features_top_missing_ratio = train_features_missing_ratio.sort_values(ascending=False)\n",
    "print(\"missing ratio in train_features:\\n\")\n",
    "print(train_features_top_missing_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing ratio in train_features:\n",
      "\n",
      "ps_car_03_cat    0.692145\n",
      "ps_car_05_cat    0.449812\n",
      "ps_reg_03        0.182043\n",
      "ps_car_14        0.072344\n",
      "ps_car_07_cat    0.019018\n",
      "ps_ind_05_cat    0.009324\n",
      "ps_car_09_cat    0.000932\n",
      "ps_ind_02_cat    0.000294\n",
      "ps_car_01_cat    0.000176\n",
      "ps_ind_04_cat    0.000134\n",
      "ps_car_02_cat    0.000025\n",
      "ps_car_11        0.000017\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ratio of missing values in test_features\n",
    "test_features_missing_ratio = test_features.isnull().mean()\n",
    "test_features_missing_ratio = test_features_missing_ratio[test_features_missing_ratio > 0] # only consider features with non-zero missing ratio\n",
    "test_features_top_missing_ratio = test_features_missing_ratio.sort_values(ascending=False)\n",
    "print(\"missing ratio in train_features:\\n\")\n",
    "print(test_features_top_missing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features `ps_car_03_cat`, `ps_car_05_cat` and `ps_reg_03` have too high missing ratio in both training and testing set, directedly delete these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\n",
    "\n",
    "# record dropped columns for later K-S statistic calculation\n",
    "dropped_ps_car_03_cat = train_features['ps_car_03_cat']\n",
    "dropped_ps_car_05_cat = train_features['ps_car_05_cat']\n",
    "\n",
    "train_features = train_features.drop(columns=columns_to_drop, errors='ignore') # ignore error if columns to delete don't exist\n",
    "test_features = test_features.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other features have missing ratio lower than 0.08. Fill continuous features with medieans of these features. Fill categorical and binary features with modes of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of features with low missing radio\n",
    "columns_to_fill_with_mode = ['ps_car_07_cat', 'ps_ind_05_cat', 'ps_car_09_cat',\n",
    "                             'ps_ind_02_cat', 'ps_car_01_cat', 'ps_ind_04_cat',\n",
    "                             'ps_car_02_cat']\n",
    "columns_to_fill_with_median = ['ps_car_14', 'ps_car_11', 'ps_car_12', 'ps_reg_03']\n",
    "\n",
    "# training set\n",
    "train_features[columns_to_fill_with_mode] = train_features[columns_to_fill_with_mode].fillna(\n",
    "    train_features[columns_to_fill_with_mode].mode().iloc[0]\n",
    ")\n",
    "train_features[columns_to_fill_with_median] = train_features[columns_to_fill_with_median].fillna(\n",
    "    train_features[columns_to_fill_with_median].median()\n",
    ")\n",
    "\n",
    "\n",
    "# testing set\n",
    "test_features[columns_to_fill_with_mode] = test_features[columns_to_fill_with_mode].fillna(\n",
    "    test_features[columns_to_fill_with_mode].mode().iloc[0]\n",
    ")\n",
    "test_features[columns_to_fill_with_median] = test_features[columns_to_fill_with_median].fillna(\n",
    "    test_features[columns_to_fill_with_median].median()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 K-S static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating K-S static has no requirement of standardization and encoding. So dataframes `train_features`, obtained in section 1.2 in this notebook, can be used here.\n",
    "\n",
    "Calculating K-S static requires label information. So dataframes `train_label`, obtained in section 1.1 in this notebook, can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature_name  ks_statistic  max_diff_feature_value\n",
      "32       ps_car_13      0.110406                0.832079\n",
      "20       ps_reg_03      0.092526                0.820823\n",
      "5    ps_ind_06_bin      0.089346                0.000000\n",
      "19       ps_reg_02      0.087222                0.500000\n",
      "6    ps_ind_07_bin      0.077938                0.000000\n",
      "31       ps_car_12      0.077305                0.374166\n",
      "15   ps_ind_16_bin      0.072481                1.000000\n",
      "23   ps_car_04_cat      0.069395                0.000000\n",
      "34       ps_car_15      0.068128                3.162278\n",
      "21   ps_car_01_cat      0.067624                7.000000\n",
      "16   ps_ind_17_bin      0.067192                1.000000\n",
      "22   ps_car_02_cat      0.063740                0.000000\n",
      "18       ps_reg_01      0.058091                0.600000\n",
      "14       ps_ind_15      0.055723                8.000000\n",
      "4    ps_ind_05_cat      0.051454                0.000000\n",
      "0        ps_ind_01      0.050644                2.000000\n",
      "2        ps_ind_03      0.045353                4.000000\n",
      "26   ps_car_08_cat      0.040323                0.000000\n",
      "29   ps_car_11_cat      0.036227              104.000000\n",
      "33       ps_car_14      0.035675                0.398623\n",
      "27   ps_car_09_cat      0.031319                0.000000\n",
      "24   ps_car_06_cat      0.030580               11.000000\n",
      "7    ps_ind_08_bin      0.026983                0.000000\n",
      "3    ps_ind_04_cat      0.024911                0.000000\n",
      "25   ps_car_07_cat      0.020685                0.000000\n",
      "8    ps_ind_09_bin      0.015575                1.000000\n",
      "1    ps_ind_02_cat      0.014455                1.000000\n",
      "30       ps_car_11      0.013685                2.000000\n",
      "17   ps_ind_18_bin      0.008503                0.000000\n",
      "13       ps_ind_14      0.008097                0.000000\n",
      "37      ps_calc_03      0.007673                0.100000\n",
      "35      ps_calc_01      0.007267                0.300000\n",
      "36      ps_calc_02      0.006962                0.100000\n",
      "40      ps_calc_06      0.006594                7.000000\n",
      "41      ps_calc_07      0.006042                2.000000\n",
      "45      ps_calc_11      0.005591                8.000000\n",
      "42      ps_calc_08      0.005536                8.000000\n",
      "43      ps_calc_09      0.005289                2.000000\n",
      "48      ps_calc_14      0.005043                8.000000\n",
      "50  ps_calc_16_bin      0.004656                0.000000\n",
      "46      ps_calc_12      0.004639                0.000000\n",
      "47      ps_calc_13      0.004432                4.000000\n",
      "44      ps_calc_10      0.004321                9.000000\n",
      "11   ps_ind_12_bin      0.004301                1.000000\n",
      "38      ps_calc_04      0.003976                2.000000\n",
      "51  ps_calc_17_bin      0.003341                0.000000\n",
      "39      ps_calc_05      0.003337                2.000000\n",
      "54  ps_calc_20_bin      0.002389                0.000000\n",
      "52  ps_calc_18_bin      0.001787                1.000000\n",
      "53  ps_calc_19_bin      0.001677                0.000000\n",
      "49  ps_calc_15_bin      0.000823                1.000000\n",
      "10   ps_ind_11_bin      0.000627                1.000000\n",
      "12   ps_ind_13_bin      0.000495                0.000000\n",
      "9    ps_ind_10_bin      0.000270                1.000000\n",
      "28   ps_car_10_cat      0.000045                0.000000\n"
     ]
    }
   ],
   "source": [
    "ks_results = []\n",
    "\n",
    "for feature_name in train_features.columns:\n",
    "    # construct dataframe\n",
    "    temp_data = {\n",
    "        'feature': list(train_features[feature_name].values),\n",
    "        'target': list(train_label.values)\n",
    "    }\n",
    "    temp_df = pd.DataFrame(temp_data)\n",
    "    temp_df = temp_df.sort_values(by='feature')\n",
    "    # calculate ks_statistic of the given feature\n",
    "    if 'cat' not in str(feature_name) and 'bin' not in str(feature_name):\n",
    "        temp_df['cum_positive'] = (temp_df['target'] == 1).cumsum() / (temp_df['target'] == 1).sum() # cdf of label 1\n",
    "        temp_df['cum_negative'] = (temp_df['target'] == 0).cumsum() / (temp_df['target'] == 0).sum() # cdf of label 0\n",
    "        temp_df['diff'] = np.abs(temp_df['cum_positive'] - temp_df['cum_negative'])\n",
    "        ks_statistic = temp_df['diff'].max()\n",
    "        max_diff_feature_value = temp_df.loc[temp_df['diff'].idxmax(), 'feature'] # value of the continuous feature at which maximum cdf difference is achieved\n",
    "    else:\n",
    "        category_binary_stats = temp_df.groupby('feature')['target'].value_counts(normalize=False).unstack(fill_value=0)\n",
    "        label_counts = temp_df['target'].value_counts()\n",
    "        category_binary_stats[0] = category_binary_stats[0] / label_counts[0]\n",
    "        category_binary_stats[1] = category_binary_stats[1] / label_counts[1]\n",
    "        category_binary_stats['abs_diff'] = (category_binary_stats[0] - category_binary_stats[1]).abs()\n",
    "        ks_statistic = category_binary_stats['abs_diff'].max()\n",
    "        max_diff_feature_value = category_binary_stats['abs_diff'].idxmax() # value of the categorical/binary feature at which maximum proportion difference is achieved\n",
    "\n",
    "    ks_results.append({'feature_name': feature_name, 'ks_statistic': ks_statistic, 'max_diff_feature_value': max_diff_feature_value})\n",
    "\n",
    "ks_df = pd.DataFrame(ks_results)\n",
    "ks_df_sorted = ks_df.sort_values(by='ks_statistic', ascending=False)\n",
    "print(ks_df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate K-S statistics of dropped features: `ps_car_03_cat` and `ps_car_05_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-S Statistic of dropped_ps_car_03_cat:  0.06447520236299445\n",
      "K-S Statistic of dropped_ps_car_05_cat:  0.03492610362113624\n"
     ]
    }
   ],
   "source": [
    "# dropped_ps_car_03_cat\n",
    "temp_data = {\n",
    "    'feature': list(dropped_ps_car_03_cat.values),\n",
    "    'target': list(train_label.values)\n",
    "}\n",
    "temp_df = pd.DataFrame(temp_data)\n",
    "temp_df = temp_df.sort_values(by='feature')\n",
    "category_stats = temp_df.groupby('feature')['target'].value_counts(normalize=False).unstack(fill_value=0)\n",
    "label_counts = temp_df['target'].value_counts()\n",
    "category_stats[0] = category_stats[0] / label_counts[0]\n",
    "category_stats[1] = category_stats[1] / label_counts[1]\n",
    "category_stats['abs_diff'] = (category_stats[0] - category_stats[1]).abs()\n",
    "ks_statistic = category_stats['abs_diff'].max()\n",
    "print(\"K-S Statistic of dropped_ps_car_03_cat: \", ks_statistic)\n",
    "\n",
    "# dropped_ps_car_05_cat\n",
    "temp_data = {\n",
    "    'feature': list(dropped_ps_car_05_cat.values),\n",
    "    'target': list(train_label.values)\n",
    "}\n",
    "temp_df = pd.DataFrame(temp_data)\n",
    "temp_df = temp_df.sort_values(by='feature')\n",
    "category_stats = temp_df.groupby('feature')['target'].value_counts(normalize=False).unstack(fill_value=0)\n",
    "label_counts = temp_df['target'].value_counts()\n",
    "category_stats[0] = category_stats[0] / label_counts[0]\n",
    "category_stats[1] = category_stats[1] / label_counts[1]\n",
    "category_stats['abs_diff'] = (category_stats[0] - category_stats[1]).abs()\n",
    "ks_statistic = category_stats['abs_diff'].max()\n",
    "print(\"K-S Statistic of dropped_ps_car_05_cat: \", ks_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated K-S statistic is stored in a dataframe named `ks_df_sorted`. Select features with K-S statistic higher than 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_features = list(ks_df_sorted[ks_df_sorted['ks_statistic'] < 0.01]['feature_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.drop(columns=dropped_features, errors='ignore') # ignore error if columns to delete don't exist\n",
    "test_features = test_features.drop(columns=dropped_features, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previously dropped feature `ps_car_03_cat` and `ps_car_05_cat` has K-S static greater than 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 standardization and encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of continuous features\n",
    "continuous_features = [column_name for column_name in train_features.columns if '_cat' not in column_name and '_bin' not in column_name]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_continuous = scaler.fit_transform(train_features[continuous_features])\n",
    "test_continuous = scaler.transform(test_features[continuous_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical features using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of categorical features\n",
    "categorical_features = [column_name for column_name in train_features.columns if '_cat' in column_name]\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "train_categorical = encoder.fit_transform(train_features[categorical_features])\n",
    "test_categorical = encoder.transform(test_features[categorical_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary features remain fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names of binary features\n",
    "binary_features = [column_name for column_name in train_features.columns if '_bin' in column_name]\n",
    "\n",
    "train_binary = train_features[binary_features].values\n",
    "test_binary = test_features[binary_features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate three kinds of features into final traning and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_processed = np.hstack((train_continuous, train_categorical, train_binary))\n",
    "test_features_processed = np.hstack((test_continuous, test_categorical, test_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables `train_features_processed` and `test_features_processed` are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (476169, 185))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_features_processed), train_features_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (119043, 185))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_features_processed), test_features_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. data resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_features_processed` derived in section 1.4 in this notebook can be used here. Only `train_features_processed` should be resampled. Resampling `test_features_processed` will lead to wrong testing AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, (476169, 185))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_processed = pd.DataFrame(train_features_processed)\n",
    "type(train_features_processed), train_features_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.series.Series, (476169,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_label), train_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_features_processed` and `train_label` are first divided into a 80% development set (dev_features, dev_label) and a 20% validation set (val_features, val_label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features, val_features, dev_label, val_label = train_test_split(train_features_processed, train_label, test_size = 0.2, stratify = train_label, random_state = 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 under-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interatively undersampling majority label samples, training the logistic regression model/random forest model on the undersampled development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sampling ratio: 0.93\n",
      "Best AUC: 0.6237\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "best_auc = 0\n",
    "best_ratio = None\n",
    "performance_results = []\n",
    "majority_class = 0\n",
    "majority_class_count = dev_label.value_counts()[majority_class]\n",
    "\n",
    "# undersampling ratio\n",
    "step = 0.07\n",
    "ratios = np.arange(1.0, 0.0, -step)\n",
    "\n",
    "for ratio in ratios:\n",
    "    # RandomUnderSampler\n",
    "    rus = RandomUnderSampler(sampling_strategy={majority_class: int(ratio * majority_class_count)}, random_state=42)\n",
    "    dev_features_resampled, dev_label_resampled = rus.fit_resample(dev_features, dev_label)\n",
    "\n",
    "    # train logistic regression/random forest on the resampled data\n",
    "    #model = LogisticRegression(max_iter=100, class_weight='balanced', C=1e-2, penalty='l2', solver='lbfgs')\n",
    "    #model.fit(dev_features_resampled, dev_label_resampled)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1200, min_samples_leaf=1000, max_leaf_nodes=100, n_jobs=-1)\n",
    "    model.fit(dev_features_resampled, dev_label_resampled)\n",
    "\n",
    "    # evaluate the trained model on the validation set\n",
    "    val_label_pred_prob = model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_label, val_label_pred_prob)\n",
    "    \n",
    "    # record current performance and update best performance\n",
    "    performance_results.append((ratio, auc))\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_ratio = ratio\n",
    "\n",
    "# show best performance and corresponded ratio\n",
    "print(f\"Best sampling ratio: {best_ratio:.2f}\")\n",
    "print(f\"Best AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 over-sampling\n",
    "\n",
    "Interatively oversampling minority label samples, training the logistic regression model/random forest model on the oversampled development set, until the model's performance deteriorates on the validation set.\n",
    "\n",
    "Oversampling is based on the undersampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class = 0\n",
    "majority_class_count = dev_label.value_counts()[majority_class]\n",
    "rus = RandomUnderSampler(sampling_strategy={majority_class: int(0.93 * majority_class_count)}, random_state=42)\n",
    "dev_features_undersampled, dev_label_undersampled = rus.fit_resample(dev_features, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sampling ratio: 1.00\n",
      "Best AUC: 0.6239\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "best_auc = 0\n",
    "best_ratio = None\n",
    "performance_results = []\n",
    "minority_class = 1\n",
    "minority_class_count = dev_label.value_counts()[minority_class]\n",
    "\n",
    "# oversampling ratio\n",
    "step = 0.07\n",
    "ratios = np.arange(1.0, 2.0, step)\n",
    "\n",
    "for ratio in ratios:\n",
    "    # RandomOverSampler\n",
    "    rus = RandomOverSampler(sampling_strategy={minority_class: int(ratio * minority_class_count)}, random_state=42)\n",
    "    dev_features_resampled, dev_label_resampled = rus.fit_resample(dev_features_undersampled, dev_label_undersampled) # oversampling bases on the undersampled data\n",
    "\n",
    "    # train logistic regression on the resampled data\n",
    "    #model = LogisticRegression(max_iter=100, class_weight='balanced', C=1e-2, penalty='l2', solver='lbfgs')\n",
    "    #model.fit(dev_features_resampled, dev_label_resampled)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1200, min_samples_leaf=1000, max_leaf_nodes=100, n_jobs=-1)\n",
    "    model.fit(dev_features_resampled, dev_label_resampled)\n",
    "\n",
    "    # evaluate the trained model on the validation set\n",
    "    val_label_pred_prob = model.predict_proba(val_features)[:, 1]\n",
    "    auc = roc_auc_score(val_label, val_label_pred_prob)\n",
    "    \n",
    "    # record current performance and update best performance\n",
    "    performance_results.append((ratio, auc))\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_ratio = ratio\n",
    "\n",
    "# show best performance and corresponded ratio\n",
    "print(f\"Best sampling ratio: {best_ratio:.2f}\")\n",
    "print(f\"Best AUC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 model training\n",
    "\n",
    "Based on the undersampling ratio and oversampling ratio obtained in the last section, undersample and oversample training features. Then train a logistic regression model and a raondom forest model on the resampled train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample majority class\n",
    "majority_class = 0\n",
    "majority_class_count = train_label.value_counts()[majority_class]\n",
    "rus = RandomUnderSampler(sampling_strategy={majority_class: int(0.93 * majority_class_count)}, random_state=42)\n",
    "train_features_processed_resampled, train_label_resampled = rus.fit_resample(train_features_processed, train_label)\n",
    "\n",
    "# oversample minority class\n",
    "minority_class = 1\n",
    "minority_class_count = train_label_resampled.value_counts()[minority_class]\n",
    "rus = RandomOverSampler(sampling_strategy={minority_class: int(1.0 * minority_class_count)}, random_state=42)\n",
    "train_features_processed_resampled, train_label_resampled = rus.fit_resample(train_features_processed_resampled, train_label_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444052, 185)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_processed_resampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight='balanced')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a logistic regression model on the resampled data\n",
    "lr_params = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 1e1],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=100, class_weight='balanced', C=1e-2, penalty='l2', solver='lbfgs')\n",
    "lr_model.fit(train_features_processed_resampled, train_label_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=100, min_samples_leaf=1000,\n",
       "                       n_estimators=1200, n_jobs=-1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a random forest model on the resampled data\n",
    "rf_model = RandomForestClassifier(n_estimators=1200, min_samples_leaf=1000, max_leaf_nodes=100, n_jobs=-1)\n",
    "rf_model.fit(train_features_processed_resampled, train_label_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of logistic regression: 0.6270988941593839\n",
      "AUC of random forest: 0.6275041779330105\n"
     ]
    }
   ],
   "source": [
    "lr_model_pred_prob = lr_model.predict_proba(test_features_processed)[:, 1]\n",
    "auc_score_lr = roc_auc_score(test_label, lr_model_pred_prob)\n",
    "print(f\"AUC of logistic regression: {auc_score_lr}\")\n",
    "\n",
    "rf_model_pred_prob = rf_model.predict_proba(test_features_processed)[:, 1]\n",
    "auc_score_rf = roc_auc_score(test_label, rf_model_pred_prob)\n",
    "print(f\"AUC of random forest: {auc_score_rf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
